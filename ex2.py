# -*- coding: utf-8 -*-
"""Regression_&_Regularization_Exercise.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13cjnJXcdExSIwEV1ZsLzd_Pmbibi6CG8

# Yandex Data Science School
## Linear Regression & Regularization Exercise.


## Outline
In this exercise you will learn the following topics:

1. Refresher on how linear regression is solved in batch and in Gradient Descent
2. Implementation of Ridge Regression
3. Comparing Ridge, Lasso and vanila Linear Regression on a dataset
"""

from sklearn.model_selection import train_test_split
import torch
from sklearn.datasets import load_boston
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import ShuffleSplit
from scipy import stats
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

import warnings

warnings.filterwarnings("ignore", category=FutureWarning)

"""# Git Exercise
In this exercise you will also experience working with github.

You might need to install local python enviroment.
Installation Instruction for ex2 - working on a local python environment:
https://docs.google.com/document/d/1G0rBo36ff_9JzKy0EkCalK4m_ThNUuJ2bRz463EHK9I

## please add the github link of your work below:

example: https://github.com/username/exercise_name

## Refresher on Ordinary Least Square (OLS) aka Linear Regeression

### Lecture Note

In Matrix notation, the matrix $X$ is of dimensions $n \times p$ where each row is an example and each column is a feature dimension. 

Similarily, $y$ is of dimension $n \times 1$ and $w$ is of dimensions $p \times 1$.

The model is $\hat{y}=X\cdot w$ where we assume for simplicity that $X$'s first columns equals to 1 (one padding), to account for the bias term.

Our objective is to optimize the loss $L$ defines as resiudal sum of squares (RSS): 

$L_{RSS}=\frac{1}{N}\left\Vert Xw-y \right\Vert^2$ (notice that in matrix notation this means summing over all examples, so $L$ is scalar.)

To find the optimal $w$ one needs to derive the loss with respect to $w$.

$\frac{\partial{L_{RSS}}}{\partial{w}}=\frac{2}{N}X^T(Xw-y)$ (to see why, read about [matrix derivatives](http://www.gatsby.ucl.ac.uk/teaching/courses/sntn/sntn-2017/resources/Matrix_derivatives_cribsheet.pdf) or see class notes )

Thus, the gardient descent solution is $w'=w-\alpha \frac{2}{N}X^T(Xw-y)$.

Solving $\frac{\partial{L_{RSS}}}{\partial{w}}=0$ for $w$ one can also get analytical solution:

$w_{OLS}=(X^TX)^{-1}X^Ty$

The first term, $(X^TX)^{-1}X^T$ is also called the pseudo inverse of $X$.

See [lecture note from Stanford](https://web.stanford.edu/~mrosenfe/soc_meth_proj3/matrix_OLS_NYU_notes.pdf) for more details.

## Exercise 1 - Ordinary Least Square
* Get the boston housing dataset by using the scikit-learn package. hint: [load_boston](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html)

* What is $p$? what is $n$ in the above notation? hint: [shape](https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.ndarray.shape.html)

* write a model `OrdinaryLinearRegression` which has a propoery $w$ and 3 methods: `fit`, `predict` and `score` (which returns the MSE on a given sample set). Hint: use [numpy.linalg.pinv](https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.linalg.pinv.html) to be more efficient.

* Fit the model. What is the training MSE?

* Plot a scatter plot where on x-axis plot $Y$ and in the y-axis $\hat{Y}_{OLS}$

* Split the data to 75% train and 25% test 20 times. What is the average MSE now for train and test? Hint: use [train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) or [ShuffleSplit](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html).

* Use a t-test to proove that the MSE for training is significantly smaller than for testing. What is the p-value? Hint: use [scipy.stats.ttest_rel](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.ttest_rel.html). 

* Write a new class `OrdinaryLinearRegressionGradientDescent` which inherits from `OrdinaryLinearRegression` and solves the problem using gradinet descent. The class should get as a parameter the learning rate and number of iteration. Plot the class convergance. What is the effect of learning rate? How would you find number of iteration automatically? Note: Gradient Descent does not work well when features are not scaled evenly (why?!). Be sure to normalize your features first.

* The following parameters are optional (not mandatory to use):
    * early_stop - True / False boolean to indicate to stop running when loss stops decaying and False to continue.
    * verbose- True/False boolean to turn on / off logging, e.g. print details like iteration number and loss (https://en.wikipedia.org/wiki/Verbose_mode)
    * track_loss - True / False boolean when to save loss results to present later in learning curve graphs
"""


# * write a model `Ols` which has a propoery $w$ and 3 methods: `fit`, `predict` and `score`.? hint: use [numpy.linalg.pinv](https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.linalg.pinv.html) to be more efficient.

class Ols(object):
    def __init__(self):
        self.w = None

    @staticmethod
    def pad(X):
        return np.pad(X, ((0, 0), (1, 0)), constant_values=1)

    def fit(self, X, Y):
        return self._fit(X, Y)

    def _fit(self, X, Y):
        X = Ols.pad(X)
        covariance = np.dot(X.T, X)
        correlation = np.dot(X.T, Y)
        weights = np.dot(np.linalg.pinv(covariance), correlation)
        self.w = weights

    def predict(self, X):
        # return wx
        return self._predict(X)

    def _predict(self, X):
        # optional to use this
        # we use padding inside the _predict otherwise we will normalize the bias padding
        X = Ols.pad(X)
        return np.dot(X, self.w)

    def score(self, X, Y):
        predicted_labels = self.predict(X)
        if isinstance(predicted_labels, torch.Tensor):
            predicted_labels = predicted_labels.detach().numpy()

        return mean_squared_error(Y, predicted_labels)


boston_X, boston_y = load_boston(return_X_y=True)

number_sample = boston_X.shape[0]
number_features = boston_X.shape[1]

print(f"Number of samples {number_sample}")
print(f"Number of features {number_features}")

# split to train test

X_train, X_test, y_train, y_test = train_test_split(boston_X, boston_y, test_size=0.25, random_state=42)

# train / test MSE
ols_model = Ols()
ols_model.fit(X_train, y_train)
train_score = ols_model.score(X_train, y_train)
test_score = ols_model.score(X_test, y_test)
train_predictions = ols_model.predict(X_train)
test_predictions = ols_model.predict(X_test)
print(f"OLS train score {train_score}")
print(f"OLS test score {test_score}")

plt.figure(figsize=(8, 8))
sns.scatterplot(x=y_train, y=train_predictions)
sns.scatterplot(x=y_test, y=test_predictions)
plt.xlabel("true values")
plt.ylabel("predicted values")
plt.title("Boston dataset predicted values vs true values")
plt.legend(["train", "test"])

ss = ShuffleSplit(n_splits=20, test_size=.25, random_state=42)
mse_scores_train = []
mse_scores_test = []

for i, (train_index, test_index) in enumerate(ss.split(boston_X)):
    train_x, train_y = boston_X[train_index], boston_y[train_index]
    test_x, test_y = boston_X[test_index], boston_y[test_index]
    ols_model = Ols()
    ols_model.fit(train_x, train_y)
    train_score = ols_model.score(train_x, train_y)
    test_score = ols_model.score(test_x, test_y)
    mse_scores_train.append(train_score)
    mse_scores_test.append(test_score)

np.mean(mse_scores_train), np.mean(mse_scores_test)

statistcs, pvalue = stats.ttest_rel(mse_scores_train, mse_scores_test)

"""We can see significant difference between train result and test results. pvalue is small 0.03

Before we choose normalization method, we need to find the distribution of each feature on the data set.
"""

fig, axs = plt.subplots(1, boston_X.shape[1])
fig.set_size_inches(130, 8)
for feature in range(boston_X.shape[1]):
    plt.figure(1, figsize=(10, 8))
    plt.subplot(1, boston_X.shape[1], feature + 1)
    feature_data = boston_X[:, feature]
    sns.histplot(x=feature_data, stat='density', kde=True, bins=20)
    plt.title(f"Histogram plot for feature X{feature}")
    plt.xlabel(f"X{feature}")
    plt.ylabel("Density")

plt.figure(figsize=(8, 8))
sns.histplot(x=boston_X[:, 5], stat='density', kde=True, bins=20)
plt.title("Histogram plot for feature X5")
plt.xlabel("X5")
plt.ylabel("Density")

"""We can see that feature X5 (feature number 6) is well fits normal distribution. 

But because we normalize the data inside the model (that is bad coupling), we will use Min-Max normilization to all data.
"""


# Write a new class OlsGd which solves the problem using gradinet descent.
# The class should get as a parameter the learning rate and number of iteration.
# Plot the loss convergance. for each alpha, learning rate plot the MSE with respect to number of iterations.
# What is the effect of learning rate?
# How would you find number of iteration automatically?
# Note: Gradient Descent does not work well when features are not scaled evenly (why?!). Be sure to normalize your feature first.

class Normalizer(object):
    def __init__(self, min_max_range=(0, 1)):
        self.data_min = None
        self.data_max = None
        self.range = min_max_range

    def fit(self, X):
        self.data_min = np.min(X, axis=0)
        self.data_max = np.max(X, axis=0)
        return self.predict(X)

    def predict(self, X):
        # apply normalization
        # because we don't get params to the
        return (X - self.data_min) / (self.data_max - self.data_min) * (self.range[1] - self.range[0])


class OlsGd(Ols):

    def __init__(self, learning_rate=.05,
                 num_iteration=1000,
                 normalize=True,
                 early_stop=True,
                 verbose=True):

        super(OlsGd, self).__init__()
        self.learning_rate = learning_rate
        self.num_iteration = num_iteration
        self.early_stop = early_stop
        self.normalize = normalize
        self.normalizer = Normalizer()
        self.verbose = verbose
        self.loss_history = None

    def _fit(self, X, Y, reset=True, track_loss=True):
        # remeber to normalize the data before starting
        if self.normalize:
            X = self.normalizer.fit(X)
        X = OlsGd.pad(X)

        X = torch.from_numpy(X.astype(np.float32))
        Y = torch.from_numpy(Y.astype(np.float32))
        self.w = torch.randn(X.shape[1], 1, requires_grad=True)
        epochs_loss = []
        for epoch in range(self.num_iteration):

            step_loss = self._step(X, Y)
            epochs_loss.append(step_loss)
            if self.verbose:
                print(f"Epoch {epoch}: loss = {epochs_loss[epoch]}")

            if self.early_stop and epoch >= 2:
                if abs(epochs_loss[epoch - 1] - epochs_loss[epoch]) <= 1e-5:
                    if self.verbose:
                        print(f"Early stopping condition met, number of iteration {epoch}")
                    break

        if track_loss:
            self.loss_history = epochs_loss

    def _predict(self, X):
        if self.normalize:
            X = self.normalizer.predict(X)
        X = OlsGd.pad(X)
        X = torch.from_numpy(X.astype(np.float32))
        return torch.mm(X, self.w).flatten()

    def _step(self, X, Y):
        # use w update for gradient descent
        y_pred = torch.mm(X, self.w).flatten()

        loss = torch.square(y_pred - Y).mean()
        loss.backward()

        with torch.no_grad():
            self.w.sub_(self.learning_rate * self.w.grad)
        self.w.grad.zero_()
        return loss.item()


torch.manual_seed(42)
olsgd = OlsGd(num_iteration=100000, learning_rate=0.01, verbose=False)
olsgd.fit(X_train, y_train)
train_score = olsgd.score(X_train, y_train)
test_score = olsgd.score(X_test, y_test)

print(f"OlsGd train score: {train_score}")
print(f"OlsGd test score: {test_score}")

plt.figure(figsize=(12, 6))
sns.lineplot(y=olsgd.loss_history, x=np.arange(1, len(olsgd.loss_history) + 1))
plt.xlabel("Epoch")
plt.ylabel("Error")
plt.title("OlsGd error by epoch")

# What is the effect of learning rate?
learning_rates = [0.001, 0.01, 0.05, 0.1]
lr_results = {}
for lr in learning_rates:
    olsgd = OlsGd(num_iteration=1000, learning_rate=lr, verbose=False, early_stop=False)
    olsgd.fit(boston_X, boston_y)
    lr_results[lr] = olsgd.loss_history

lr_results = pd.DataFrame(lr_results)

plt.figure(figsize=(12, 8))

sns.lineplot(data=lr_results)
plt.title("Learning Rate Effect")
plt.ylabel("Mean square error")
plt.xlabel("epoch")
plt.ylim(0, 500)
plt.legend()

"""**What is the effect of learning rate? How would you find number of iteration automatically?**

We can check how much the model is improves between each iteration, if the model improvment is less than defines threshold we stop the training.
This is better solution than using fix number of iterations because when we reduce the learning rate, we may need more iterations to converge. Whem we use high learning rate we may converge fater than the num of iterations, and it's waste of computational resources.
We can see from the plot that for learning rate 0.001, the model didn't converge in 1000 iterations, in addition for learning rate 0.1, the model converge much faster than 1000 iterations.

## Exercise 2 - Ridge Linear Regression

Recall that ridge regression is identical to OLS but with a L2 penalty over the weights:

$L(y,\hat{y})=\sum_{i=1}^{i=N}{(y^{(i)}-\hat{y}^{(i)})^2} + \lambda \left\Vert w \right\Vert_2^2$

where $y^{(i)}$ is the **true** value and $\hat{y}^{(i)}$ is the **predicted** value of the $i_{th}$ example, and $N$ is the number of examples

* Show, by differentiating the above loss, that the analytical solution is $w_{Ridge}=(X^TX+\lambda I)^{-1}X^Ty$
* Change `OrdinaryLinearRegression` and `OrdinaryLinearRegressionGradientDescent` classes to work also for ridge regression (do not use the random noise analogy but use the analytical derivation). Either add a parameter, or use inheritance.
* **Bonus: Noise as a regularizer**: Show that OLS (ordinary least square), if one adds multiplicative noise to the features the **average** solution for $W$ is equivalent to Ridge regression. In other words, if $X'= X*G$ where $G$ is an uncorrelated noise with variance $\sigma$ and mean 1, then solving for $X'$ with OLS is like solving Ridge for $X$. What is the interpretation?

**ANALYTICAL SOLUTION**

![image.png](data:image/png;base64,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)
"""


class RidgeLs(Ols):
    def __init__(self, ridge_lambda, *wargs, **kwargs):
        super(RidgeLs, self).__init__(*wargs, **kwargs)
        self.ridge_lambda = ridge_lambda

    def _fit(self, X, Y):
        X = RidgeLs.pad(X)
        covariance = np.dot(X.T, X) + np.identity(X.shape[1]) * self.ridge_lambda
        correlation = np.dot(X.T, Y)
        weights = np.dot(np.linalg.pinv(covariance), correlation)
        self.w = weights


ridgels = RidgeLs(ridge_lambda=1e-2)
ridgels.fit(X_train, y_train)
train_score = ridgels.score(X_train, y_train)
test_score = ridgels.score(X_test, y_test)

print(f"RidgeLs train score: {train_score}")
print(f"RidgeLs test score: {test_score}")

"""### Use scikitlearn implementation for OLS, Ridge and Lasso"""

from sklearn.linear_model import LinearRegression, Lasso, Ridge

X_train, X_test, y_train, y_test = train_test_split(boston_X, boston_y, test_size=0.25, random_state=42)


def model_evaluation(model, train_x, test_x, train_y, test_y):
    model.fit(train_x, train_y)
    model_train_prediction = model.predict(train_x)
    model_train_mse = mean_squared_error(train_y, model_train_prediction)
    model_test_prediction = model.predict(test_x)
    model_test_mse = mean_squared_error(test_y, model_test_prediction)
    return model_train_mse, model_test_mse


linear_regression = LinearRegression()
train_mse, test_mse = model_evaluation(linear_regression, X_train, X_test, y_train, y_test)

print(f"LinearRegression Train MSE: {train_mse}")
print(f"LinearRegression Test MSE: {test_mse}")

# Lasso
lasso = Lasso()
train_mse, test_mse = model_evaluation(lasso, X_train, X_test, y_train, y_test)

print(f"Lasso Train MSE: {train_mse}")
print(f"Lasso Test MSE: {test_mse}")

# Ridge
ridge = Ridge()
train_mse, test_mse = model_evaluation(ridge, X_train, X_test, y_train, y_test)

print(f"Ridge Train MSE: {train_mse}")
print(f"Ridge Test MSE: {test_mse}")
